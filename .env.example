# ===========================================
# Open Gemini CLI - Environment Configuration
# ===========================================
# Copy to .env and configure your values

# ----- Authentication -----
# Set to "true" to skip Google auth (required for local LLMs)
BYPASS_AUTH=true

# ----- LLM Provider Selection -----
# Options: local-mlx, llamacpp, vllm, gpt-5, gpt-5-mini, gpt-4.1-mini,
#          codex-mini-latest, claude-opus, claude-sonnet, claude-haiku,
#          gemini-flash, gemini-3-pro
LLM_PROVIDER=local-mlx

# ----- Local LLM Server URLs -----
# MLX Server (macOS Apple Silicon)
KAIDEX_SERVER_URL=http://localhost:11435

# llama.cpp Server
LLAMACPP_SERVER_URL=http://localhost:8080

# vLLM Server
VLLM_SERVER_URL=http://localhost:8000

# ----- Cloud Provider API Keys -----
# OpenAI (GPT-4, GPT-5, Codex)
OPENAI_API_KEY=

# Anthropic (Claude)
ANTHROPIC_API_KEY=

# Google Gemini
GEMINI_API_KEY=

# ----- Web Search (Optional) -----
# Tavily AI Search - recommended for local LLMs
# Free 1000 searches/month: https://tavily.com
TAVILY_API_KEY=

# Google Custom Search API (fallback)
# Enable API: https://console.cloud.google.com/apis/library/customsearch.googleapis.com
# Create engine: https://programmablesearchengine.google.com/
GOOGLE_API_KEY=
GOOGLE_SEARCH_ENGINE_ID=

# ----- Bundled Extensions (Optional) -----
# Conductor: Context-driven development workflow
# Provides /conductor:setup, /conductor:newTrack, /conductor:implement commands
# Set to "true" to enable planning-first workflow
CONDUCTOR_ENABLED=false
